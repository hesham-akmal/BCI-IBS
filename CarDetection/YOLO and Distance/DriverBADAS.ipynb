{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yolo imports\n",
    "from __future__ import division\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import cv2 \n",
    "import time\n",
    "import argparse\n",
    "import os \n",
    "import os.path as osp\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import random\n",
    "from skimage import io\n",
    "from matplotlib import pyplot as plt\n",
    "from ConfidenceThresholdingAndNonMaximumSuppression import *\n",
    "from NeuralNetwork import *\n",
    "from Utilities import *\n",
    "\n",
    "#########################################################################################################################\n",
    "\n",
    "\n",
    "# Lane detection and distance estimation imports\n",
    "import glob\n",
    "from skimage import io, data\n",
    "import matplotlib.image as mpimg\n",
    "import camera_calibration\n",
    "import lane_detection\n",
    "import vanishing_point\n",
    "#import utilities\n",
    "import perspective_transform\n",
    "\n",
    "# YOLO code to be run only once ########################################################################################\n",
    "\n",
    "#Driver values\n",
    "\n",
    "images = 'imgs/'\n",
    "batch_size = 1\n",
    "confidence = 0.5\n",
    "nms_thesh = 0.4\n",
    "start = 0\n",
    "weights_file = 'yolov3.weights'\n",
    "cfg_file = 'cfg/yolov3.cfg'\n",
    "\n",
    "reso = 288\n",
    "det = 'det/'\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "num_classes = 80    #For COCO\n",
    "classes = load_classes(\"data/coco.names\")\n",
    "\n",
    "######\n",
    "\n",
    "#Set up the neural network\n",
    "print(\"Loading network.....\")\n",
    "model = Darknet(cfg_file)\n",
    "model.load_weights(weights_file)\n",
    "print(\"Network successfully loaded\")\n",
    "\n",
    "model.net_info[\"height\"] = reso\n",
    "inp_dim = int(model.net_info[\"height\"])\n",
    "assert inp_dim % 32 == 0 \n",
    "assert inp_dim > 32\n",
    "\n",
    "#If there's a GPU availible, put the model on GPU\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "#Set the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "#########################################################################################################################\n",
    "\n",
    "# Lane detection and distance estimation code to be run only once\n",
    "\n",
    "ret, mtx, dist, rvecs, tvecs = camera_calibration.calibrate(False)\n",
    "\n",
    "#########################################################################################################################\n",
    "\n",
    "# YOLO prediction function\n",
    "# Takes BGR image and returns: 1) image annotated with bounding boxes, 2) list of top left and bottom right\n",
    "# coordinates of bounding boxes, 3) list of class numbers which correspond to class labels\n",
    "\n",
    "def get_pred(img):\n",
    "    read_dir = time.time()\n",
    "    #Detection phase\n",
    "    imlist = ['img']\n",
    "\n",
    "    if not os.path.exists(det):\n",
    "        os.makedirs(det)\n",
    "\n",
    "    load_batch = time.time()\n",
    "    loaded_ims = [cv2.undistort(np.array(img), mtx, dist, None, mtx)]\n",
    "    \n",
    "    #PyTorch Variables for images\n",
    "    im_batches = list(map(prep_image, loaded_ims, [inp_dim for x in range(len(imlist))]))\n",
    "\n",
    "    #List containing dimensions of original images\n",
    "    im_dim_list = [(x.shape[1], x.shape[0]) for x in loaded_ims]\n",
    "    im_dim_list = torch.FloatTensor(im_dim_list).repeat(1,2)\n",
    "\n",
    "    if CUDA:\n",
    "        im_dim_list = im_dim_list.cuda()\n",
    "\n",
    "    leftover = 0\n",
    "    if (len(im_dim_list) % batch_size):\n",
    "        leftover = 1\n",
    "\n",
    "    if batch_size != 1:\n",
    "        num_batches = len(imlist) // batch_size + leftover            \n",
    "        im_batches = [torch.cat((im_batches[i*batch_size : min((i +  1)*batch_size, len(im_batches))]))  for i in range(num_batches)]  \n",
    "\n",
    "    write = 0\n",
    "    start_det_loop = time.time()\n",
    "    \n",
    "    for i, batch in enumerate(im_batches):\n",
    "        #load the image \n",
    "        start = time.time()\n",
    "        t = time.time()  \n",
    "        if CUDA:\n",
    "            batch = batch.cuda()\n",
    "        with torch.no_grad():\n",
    "            prediction = model(Variable(batch), CUDA)\n",
    "#         print(\"prediction: \" , time.time()-t)    \n",
    "        t = time.time()    \n",
    "        prediction = write_results(prediction, confidence, num_classes, nms_conf = nms_thesh)\n",
    "        end = time.time()\n",
    "\n",
    "        if type(prediction) == int:\n",
    "\n",
    "            for im_num, image in enumerate(imlist[i*batch_size: min((i +  1)*batch_size, len(imlist))]):\n",
    "                im_id = i*batch_size + im_num\n",
    "#                 print(\"{0:20s} predicted in {1:6.3f} seconds\".format(image.split(\"/\")[-1], (end - start)/batch_size))\n",
    "#                 print(\"{0:20s} {1:s}\".format(\"Objects Detected:\", \"\"))\n",
    "#                 print(\"----------------------------------------------------------\")\n",
    "            continue\n",
    "\n",
    "        prediction[:,0] += i*batch_size    #transform the atribute from index in batch to index in imlist \n",
    "    \n",
    "    try:\n",
    "        if not write:                      #If we have't initialised output\n",
    "            output = prediction  \n",
    "            write = 1\n",
    "        else:\n",
    "            output = torch.cat((output,prediction))\n",
    "\n",
    "        for im_num, image in enumerate(imlist[i*batch_size: min((i +  1)*batch_size, len(imlist))]):\n",
    "            im_id = i*batch_size + im_num\n",
    "            objs = [classes[int(x[-1])] for x in output if int(x[0]) == im_id]\n",
    "#             print(\"{0:20s} predicted in {1:6.3f} seconds\".format(image.split(\"/\")[-1], (end - start)/batch_size))\n",
    "#             print(\"{0:20s} {1:s}\".format(\"Objects Detected:\", \" \".join(objs)))\n",
    "#             print(\"----------------------------------------------------------\")\n",
    "\n",
    "        if CUDA:\n",
    "            torch.cuda.synchronize()\n",
    "#         output\n",
    "    except:\n",
    "        #print(\"exception\")\n",
    "        return loaded_ims[0]\n",
    "\n",
    "    im_dim_list = torch.index_select(im_dim_list, 0, output[:,0].long())\n",
    "\n",
    "    scaling_factor = torch.min(inp_dim/im_dim_list,1)[0].view(-1,1)\n",
    "\n",
    "    output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim_list[:,0].view(-1,1))/2\n",
    "    output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim_list[:,1].view(-1,1))/2\n",
    "\n",
    "    output[:,1:5] /= scaling_factor\n",
    "\n",
    "    for i in range(output.shape[0]):\n",
    "        output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim_list[i,0])\n",
    "        output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim_list[i,1])\n",
    "\n",
    "    class_load = time.time()\n",
    "    colors = pkl.load(open(\"pallete\", \"rb\"))\n",
    "\n",
    "    draw = time.time()\n",
    "\n",
    "    list(map(lambda x: write_img(x, loaded_ims, color = random.choice(colors), classes = classes), output))\n",
    "\n",
    "    t = time.time()\n",
    "    det_names = pd.Series(imlist).apply(lambda x: \"{}/det_{}\".format(det,x.split(\"/\")[-1]))\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "#     print(\"SUMMARY\")\n",
    "###     print(\"----------------------------------------------------------\")\n",
    "###     print(\"{:25s}: {}\".format(\"Task\", \"Time Taken (in seconds)\"))\n",
    "###     print()\n",
    "###     print(\"{:25s}: {:2.3f}\".format(\"Reading addresses\", load_batch - read_dir))\n",
    "###     print(\"{:25s}: {:2.3f}\".format(\"Loading batch\", start_det_loop - load_batch))\n",
    "###     print(\"{:25s}: {:2.3f}\".format(\"Drawing Boxes\", end - draw))\n",
    "#     print(\"{:25s}: {:2.3f}\".format(\"Average time_per_img\", (end - load_batch)/len(imlist)))\n",
    "#     print(\"----------------------------------------------------------\")\n",
    "\n",
    "#     t = time.time()\n",
    "#     torch.cuda.empty_cache()\n",
    "#     print('= Time to torch.cuda.empty_cache : ', time.time() - t)\n",
    "   \n",
    "    return loaded_ims[0], output[:, 1:3], output[:, 3:5], output[:, -1]\n",
    "\n",
    "#########################################################################################################################\n",
    "\n",
    "# Perspective transform function.\n",
    "# Takes as input: 1) BGR image, 2) debug mode, 3) YOLO annotated image\n",
    "# Annotated YOLO image with lines on lanes, and returns: 1) Homography matrix, 2) x pixels per meter, 3) y pixels per meter\n",
    "\n",
    "def driver_perspective_transform(img_BGR, pred, debug=False):\n",
    "    ud_img_BGR = cv2.undistort(img_BGR, mtx, dist, None, mtx)\n",
    "    ud_img_RGB = cv2.cvtColor(ud_img_BGR, cv2.COLOR_BGR2RGB)\n",
    "    if(debug):\n",
    "        show_images([ud_img_RGB])\n",
    "    detector = lane_detection.LaneDetector()\n",
    "    lines = detector.process(ud_img_RGB, True, 0.16, debug)\n",
    "    \n",
    "    vp = vanishing_point.calculate_vanishing_point(lines, ud_img_BGR, debug)\n",
    "    \n",
    "    H, H_inv, warped = perspective_transform.perspective_transform(vp, ud_img_BGR, debug)\n",
    "    \n",
    "    x_pixels_per_meter , y_pixels_per_meter, left_low, left_high, right_low, right_high = \\\n",
    "                    perspective_transform.get_ratio(H, H_inv, warped, mtx, debug)\n",
    "    \n",
    "    # YOLO image with lanes\n",
    "    cv2.line(pred, (left_low[0], left_low[1]),(left_high[0], left_high[1]),(0,0,255),2)\n",
    "    cv2.line(pred, (right_low[0], right_low[1]),(right_high[0], right_high[1]),(0,0,255),2)\n",
    "\n",
    "    return H, x_pixels_per_meter, y_pixels_per_meter\n",
    "\n",
    "#########################################################################################################################\n",
    "\n",
    "# Distance estimation function\n",
    "# Takes as input: 1) Query pixel, 2) Bottom center coordinates of screen, 3) homography matrix,\n",
    "# 4) x pixels per meter, 5) y pixels per meter\n",
    "# Returns distance to the query pixel\n",
    "def get_distance(query_pnt, center, H, x_pixels_per_meter, y_pixels_per_meter):\n",
    "    return perspective_transform.get_distance(query_pnt, center, H, x_pixels_per_meter, y_pixels_per_meter)\n",
    "\n",
    "#######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No haptic devices found, Driving kit not connected.\n",
      "Connected!\n",
      "Client Ver:1 (Min Req: 1), Server Ver:1 (Min Req: 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing Airsim and connecting\n",
    "# Uses local airsim package (DO NOT pip install, UNINSTALL IF INSTALLED ALREADY: \"pip uninstall airsim\")\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "from ctypes import *\n",
    "sys.path.append(str(Path().resolve().parent.parent).replace('\\\\','/') + '/AirSimClient')\n",
    "os.environ['PATH'] = str(Path().resolve().parent.parent).replace('\\\\','/') + '/AirSimClient' + os.pathsep + os.environ['PATH']\n",
    "d = CDLL('SDL2.dll')\n",
    "import BADAS_fns\n",
    "BADAS_fns.Find_BADAS_Client()\n",
    "time.sleep(0.5)\n",
    "BADAS_fns.client.confirmConnection()\n",
    "\n",
    "def GetSimImg():\n",
    "    responses = BADAS_fns.client.simGetImages([ BADAS_fns.airsim.ImageRequest(\"0\", BADAS_fns.airsim.ImageType.Scene, False, False) ])\n",
    "    response = responses[0]\n",
    "    img1d = np.fromstring(response.image_data_uint8, dtype=np.uint8) \n",
    "    img = img1d.reshape(response.height, response.width, 4)  \n",
    "    return cv2.cvtColor(img, cv2.COLOR_RGB2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "dist = 0\n",
    "rel_vel = 0\n",
    "rel_acc = 0\n",
    "t = time.time()\n",
    "\n",
    "def BrakeDecide(new_dist):\n",
    "    global dist\n",
    "    global rel_vel\n",
    "    global rel_acc\n",
    "    global t\n",
    "    \n",
    "#     test using actual distance exctract from sim\n",
    "#     packet = BADAS_fns.client.getAdasPacket()  \n",
    "#     new_dist = packet[1]\n",
    "    \n",
    "    rel_vel_tmp = ( dist - new_dist ) / (time.time() - t)\n",
    "    \n",
    "    rel_acc = ( rel_vel - rel_vel_tmp) / (time.time() - t)\n",
    "    \n",
    "#     test w/o quadtratic eq\n",
    "#     rel_acc = 0\n",
    "    \n",
    "    rel_vel = rel_vel_tmp\n",
    "    dist = new_dist\n",
    "    \n",
    "    print('distance = ' , dist )\n",
    "#     print('Real rel_vel = ' , packet[3] )\n",
    "    print('rel_vel = ' , rel_vel )\n",
    "    print('rel_acc = ' , rel_acc )\n",
    "    print('rel time = ' , time.time() - t )\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    if VisionEmergencyCheck(rel_acc,rel_vel,dist):\n",
    "        print('Brake = 1')\n",
    "        BADAS_fns.StartFullBrake()\n",
    "        time.sleep(2)\n",
    "        BADAS_fns.StopBrake()\n",
    "    else:\n",
    "        print('Brake = 0')\n",
    "        \n",
    "    print('\\n')\n",
    "    \n",
    "def ThresholdCheck(T):\n",
    "    print('T for collision: ', T)\n",
    "    if(T <= 0.6 and T > 0):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# a: 0.5 * acc\n",
    "# b: vel\n",
    "# c: -distance\n",
    "def VisionEmergencyCheck(acc, vel, dist):\n",
    "    if( vel < 0.001):\n",
    "        return False\n",
    "    \n",
    "    if( acc == 0 ):\n",
    "        return ThresholdCheck(dist/vel)\n",
    "    else:\n",
    "        a = 0.5 * acc\n",
    "        b = vel\n",
    "        c = -dist\n",
    "        \n",
    "        d = (b**2) - (4*a*c)\n",
    "        if(d<0):\n",
    "#             print('No collision (d<0)')\n",
    "            return False\n",
    "        \n",
    "        T = (-b+math.sqrt(d))/(2*a)\n",
    "        return ThresholdCheck(T)\n",
    "\n",
    "# while True:\n",
    "#     time.sleep(0.1)\n",
    "#     BrakeDecide(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BADAS_fns.StopBrake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sim Real-time distance detection and showing output using CV2\n",
    "import numpy\n",
    "# Preparing text style that will be used in writing distances\n",
    "font                   = cv2.FONT_HERSHEY_SIMPLEX\n",
    "pos = (0,0)\n",
    "fontScale              = 1\n",
    "fontColor              = (255,255,255)\n",
    "lineType               = 2\n",
    "\n",
    "while True:\n",
    "    # Press \"q\" to quit\n",
    "    if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "\n",
    "#     t1 = time.time()\n",
    "    img = GetSimImg()\n",
    "#     img = GetMonitorImg()\n",
    "#     print('T Sim image extract: ' , time.time() - t1)\n",
    "        \n",
    "#     with mss.mss() as sct:\n",
    "#         monitor = {\"top\": 0, \"left\": 0, \"width\": 1920, \"height\": 1040}\n",
    "#         img = numpy.array(sct.grab(monitor))\n",
    "        \n",
    "    # Image bottom center coordinates\n",
    "    center = [img.shape[1]//2, img.shape[0]-1]\n",
    "\n",
    "    # Getting predictions from yolo\n",
    "    try:\n",
    "        t1 = time.time()\n",
    "        pred, top_left, bottom_right, labels = get_pred(img)\n",
    "        print('T Yolo: ' , time.time() - t1)\n",
    "        \n",
    "        # Perform perspective transform and annotate yolo image\n",
    "        t1 = time.time()\n",
    "        H, x_pixels_per_meter, y_pixels_per_meter = driver_perspective_transform(img, pred, False)\n",
    "        print('T driver_perspective_transform: ' , time.time() - t1)\n",
    "    except:\n",
    "        cv2.imshow(\"\", img)\n",
    "        continue\n",
    "    \n",
    "    t1 = time.time()\n",
    "    # Looping on every detected object\n",
    "    for i in range(len(labels)):\n",
    "        # Object label\n",
    "        label = classes[int(labels[i])]\n",
    "\n",
    "        if(label != \"car\"):\n",
    "            continue\n",
    "\n",
    "        # Top left x,y\n",
    "        tlx, tly = int(top_left[i,0]), int(top_left[i,1])\n",
    "\n",
    "        # Bottom right x,y\n",
    "        brx, bry = int(bottom_right[i,0]), int(bottom_right[i,1])\n",
    "\n",
    "        # Bottom center coordinates of bounding box\n",
    "        cx, cy = (tlx + brx)//2, bry\n",
    "\n",
    "        #print(str(cx) + \",\" + str(cy) + \" | \" + str(center[0]) + \",\" + str(center[1]))\n",
    "\n",
    "        # Distance to car\n",
    "        float_dist = get_distance([cx, cy], center, H, x_pixels_per_meter, y_pixels_per_meter)\n",
    "        \n",
    "#         print(type(float_dist))\n",
    "        \n",
    "        BrakeDecide(float_dist)\n",
    "\n",
    "        distance = str(round_float(float_dist))+\"m\"\n",
    "        \n",
    "        # Annotate yolo image with distance to car\n",
    "        cv2.putText(pred,\n",
    "            distance,\n",
    "            (cx-60, cy+30), \n",
    "            font, \n",
    "            fontScale,\n",
    "            fontColor,\n",
    "            lineType)\n",
    "\n",
    "        print('T Get distance: ' , time.time() - t1)\n",
    "        \n",
    "    #print(x_pixels_per_meter)\n",
    "    #print(y_pixels_per_meter)\n",
    "    #print(time.time() - t)\n",
    "    cv2.imshow(\"\", pred)\n",
    "    #show_images([cv2.cvtColor(pred, cv2.COLOR_BGR2RGB)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test one image\n",
    "# Preparing text style that will be used in writing distances\n",
    "font                   = cv2.FONT_HERSHEY_SIMPLEX\n",
    "pos = (0,0)\n",
    "fontScale              = 1\n",
    "fontColor              = (255,255,255)\n",
    "lineType               = 2\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "img = cv2.read('1202x498.JPG')\n",
    "\n",
    "# Image bottom center coordinates\n",
    "center = [img.shape[1]//2, img.shape[0]-1]\n",
    "\n",
    "#img = cv2.resize(img, dsize=(512, 256), interpolation=cv2.INTER_NEAREST) #####################RESIZING FOR FAST PREDICTION\n",
    "# Getting predictions from yolo\n",
    "try:\n",
    "    pred, top_left, bottom_right, labels = get_pred(img)\n",
    "    # Perform perspective transform and annotate yolo image\n",
    "    H, x_pixels_per_meter, y_pixels_per_meter = driver_perspective_transform(img, pred, False)\n",
    "except:\n",
    "    cv2.imshow(\"\", img)\n",
    "    continue\n",
    "\n",
    "# Looping on every detected object\n",
    "for i in range(len(labels)):\n",
    "    # Object label\n",
    "    label = classes[int(labels[i])]\n",
    "\n",
    "    if(label != \"car\"):\n",
    "        continue\n",
    "\n",
    "    # Top left x,y\n",
    "    tlx, tly = int(top_left[i,0]), int(top_left[i,1])\n",
    "\n",
    "    # Bottom right x,y\n",
    "    brx, bry = int(bottom_right[i,0]), int(bottom_right[i,1])\n",
    "\n",
    "    # Bottom center coordinates of bounding box\n",
    "    cx, cy = (tlx + brx)//2, bry\n",
    "\n",
    "    #print(str(cx) + \",\" + str(cy) + \" | \" + str(center[0]) + \",\" + str(center[1]))\n",
    "\n",
    "    # Distance to car\n",
    "    float_dist = get_distance([cx, cy], center, H, x_pixels_per_meter, y_pixels_per_meter)\n",
    "\n",
    "    distance = str(round_float(float_dist))+\"m\"\n",
    "\n",
    "    # Annotate yolo image with distance to car\n",
    "    cv2.putText(pred,\n",
    "        distance,\n",
    "        (cx-60, cy+30), \n",
    "        font, \n",
    "        fontScale,\n",
    "        fontColor,\n",
    "        lineType)\n",
    "\n",
    "#print(x_pixels_per_meter)\n",
    "#print(y_pixels_per_meter)\n",
    "print(time.time() - t)\n",
    "#cv2.imshow(\"\", cv2.cvtColor(pred, cv2.COLOR_BGR2RGB))\n",
    "show_images([cv2.cvtColor(pred, cv2.COLOR_BGR2RGB)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    while True:\n",
    "        #starttime = time.time()\n",
    "        t = time.time()\n",
    "        responses = client.simGetImages([ airsim.ImageRequest(\"0\", airsim.ImageType.Scene, False, False) ])\n",
    "        response = responses[0]\n",
    "        img1d = np.frombuffer(response.image_data_uint8, dtype=np.uint8) \n",
    "        img_rgba = img1d.reshape(response.height, response.width, 4)  \n",
    "        img_rgba = cv2.resize(img_rgba, dsize=(256, 128), interpolation=cv2.INTER_NEAREST) #####################RESIZING FOR FAST PREDICTION\n",
    "        print('= T Extract sim image: ' , time.time()-t)\n",
    "\n",
    "        t = time.time()\n",
    "        detected = get_pred(img_rgba)\n",
    "        print('= T Detecting: ' , time.time()-t)\n",
    "        \n",
    "        cv2.imshow(\"img\", detected)\n",
    "        #print('=== TOTAL TIME ' , time.time()-starttime , ' ===\\n')\n",
    "        \n",
    "        if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "except:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "x = 100\n",
    "while True:\n",
    "    x = x -1\n",
    "    if x < 0:\n",
    "        break\n",
    "    t = time.time()\n",
    "    png_image = client.simGetImage(\"1\", airsim.ImageType.Scene)\n",
    "    print('time: ' , time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BADAS_fns.Find_BADAS_Client()\n",
    "time.sleep(0.5)\n",
    "BADAS_fns.client.confirmConnection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BADAS_fns.client.simGetImages([ BADAS_fns.airsim.ImageRequest(\"0\", BADAS_fns.airsim.ImageType.Scene, False, False) ])\n",
    "response = responses[0]\n",
    "img1d = np.fromstring(response.image_data_uint8, dtype=np.uint8) \n",
    "img = img1d.reshape(response.height, response.width, 4)  \n",
    "img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print average of time to detect img and predict\n",
    "timeout = time.time() + 3 #5 secs\n",
    "\n",
    "lst = [] \n",
    "lst2 = [] \n",
    "\n",
    "while True:\n",
    "    if time.time() > timeout:\n",
    "        break\n",
    "    start = time.time()\n",
    "    responses = BADAS_fns.client.simGetImages([ BADAS_fns.airsim.ImageRequest(\"0\", BADAS_fns.airsim.ImageType.Scene, False, False) ])\n",
    "    response = responses[0]\n",
    "    img1d = np.fromstring(response.image_data_uint8, dtype=np.uint8) \n",
    "    img = img1d.reshape(response.height, response.width, 4)  \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    print(\"T SimGetImg =\", time.time() - start ) \n",
    "    lst2.append(time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    detected = get_pred(img)\n",
    "    print(\"T Yolo =\", time.time() - start ) \n",
    "    lst.append(time.time() - start)\n",
    "        \n",
    "print(\"AverageGetImage =\", sum(lst2) / len(lst2) ) \n",
    "print(\"AverageYolo =\", sum(lst) / len(lst) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#40fps fullscreen quality3 AverageAll = 0.105, AverageGetImage = 0.0021\n",
    "\n",
    "#over lan 35fps fullscreen quality3\n",
    "#AverageGetImage = 0.019696117627738725\n",
    "#AverageAll = 0.09919774414289116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timeout = time.time() + 5 #5 secs\n",
    "\n",
    "while True:\n",
    "    if time.time() > timeout:\n",
    "        break\n",
    "    x = 0\n",
    "    start = time.time()\n",
    "    while True:\n",
    "        responses = client.simGetImages([ airsim.ImageRequest(\"0\", airsim.ImageType.Scene, False, False) ])\n",
    "        response = responses[0]\n",
    "        #detected = get_pred((np.frombuffer(response.image_data_uint8, dtype=np.uint8) ).reshape(response.height, response.width, 4))\n",
    "        if(time.time() - start > 1):\n",
    "            break\n",
    "        x += 1\n",
    "\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHOW ONE IMAEG\n",
    "png_image = client.simGetImage(\"0\", airsim.ImageType.Scene)\n",
    "Image.open(io.BytesIO(png_image)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mss\n",
    "import mss.tools\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "while True:\n",
    "    \n",
    "    \n",
    "    with mss.mss() as sct:\n",
    "        for num, monitor in enumerate(sct.monitors[1:], 1):\n",
    "            # Get raw pixels from the screen\n",
    "            sct_img = sct.grab(monitor)\n",
    "            # Create the Image\n",
    "            img = Image.frombytes(\"RGB\", sct_img.size, sct_img.bgra, \"raw\", \"BGRX\")\n",
    "\n",
    "        # Grab the data\n",
    "        t = time.time()\n",
    "        sct_img = sct.grab(monitor)\n",
    "        print(time.time() - t)\n",
    "        #sct_img.\n",
    "        #print(type(sct_img))\n",
    "        # Save to the picture file\n",
    "        #mss.tools.to_png(sct_img.rgb, sct_img.size, output=output)\n",
    "        #print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = client.simGetImages([ airsim.ImageRequest(\"0\", airsim.ImageType.Scene, False, False) ])\n",
    "response = responses[0]\n",
    "img1d = np.fromstring(response.image_data_uint8, dtype=np.uint8) \n",
    "img_rgba = img1d.reshape(response.height, response.width, 4)  \n",
    "img_RGB = cv2.cvtColor(img_rgba, cv2.COLOR_RGB2BGR)\n",
    "cv2.imwrite('3.jpg',img_RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetMonitorImg():\n",
    "    with mss.mss() as sct:\n",
    "        for num, monitor in enumerate(sct.monitors[1:], 1):\n",
    "            # Get raw pixels from the screen\n",
    "            sct_img = sct.grab(monitor)\n",
    "            # Create the Image\n",
    "            return Image.frombytes(\"RGB\", sct_img.size, sct_img.bgra, \"raw\", \"BGRX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetMonitorImg2():\n",
    "    with mss.mss() as sct:\n",
    "        monitor = {\"top\": 0, \"left\": 0, \"width\": 1920, \"height\": 1040}\n",
    "        img = numpy.array(sct.grab(monitor))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
